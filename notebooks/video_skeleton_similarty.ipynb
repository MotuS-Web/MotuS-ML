{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "CONSUMER_PATH = '../videos/guide/'\n",
    "GUIDE_PATH = '../videos/guide/'\n",
    "\n",
    "keypoint_names = {\n",
    "    0: 'nose',\n",
    "    1: 'left_eye',\n",
    "    2: 'right_eye',\n",
    "    3: 'left_ear', \n",
    "    4: 'right_ear', \n",
    "    5: 'left_shoulder', \n",
    "    6: 'right_shoulder',\n",
    "    7: 'left_elbow', \n",
    "    8: 'right_elbow',\n",
    "    9: 'left_wrist', \n",
    "    10: 'right_wrist',\n",
    "    11: 'left_hip', \n",
    "    12: 'right_hip',\n",
    "    13: 'left_knee', \n",
    "    14: 'right_knee',\n",
    "    15: 'left_ankle', \n",
    "    16: 'right_ankle',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=KeypointRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KeypointRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(640, 672, 704, 736, 768, 800), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "    (keypoint_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (keypoint_head): KeypointRCNNHeads(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "    )\n",
       "    (keypoint_predictor): KeypointRCNNPredictor(\n",
       "      (kps_score_lowres): ConvTranspose2d(512, 17, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True, num_keypoints=17, progress=False)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets\n",
    "\n",
    "### Consumer datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer file lists: ../videos/guide/guide_1.mp4\n"
     ]
    }
   ],
   "source": [
    "consumer_file_lists = os.listdir(CONSUMER_PATH)\n",
    "consumer_file_lists = [os.path.join(CONSUMER_PATH, f) for f in consumer_file_lists][0]\n",
    "print(f\"Consumer file lists: {consumer_file_lists}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "consumer_video = cv2.VideoCapture(consumer_file_lists)\n",
    "consumer_frame_width = int(consumer_video.get(3))\n",
    "consumer_frame_height = int(consumer_video.get(4))\n",
    "\n",
    "consumer_video_path = CONSUMER_PATH + 'processed_videos_guide.mp4'\n",
    "consumer_video_write = cv2.VideoWriter(\n",
    "    consumer_video_path,                # output file name\n",
    "    cv2.VideoWriter_fourcc(*'MP4V'),    # codec\n",
    "    20.0,                               # fps\n",
    "    (consumer_frame_width, consumer_frame_height) # frame size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guide file list: ../videos/guide/guide_1.mp4\n"
     ]
    }
   ],
   "source": [
    "guide_file_list = os.listdir(GUIDE_PATH)\n",
    "guide_file_list = [os.path.join(GUIDE_PATH, file) for file in guide_file_list]\n",
    "guide_file_list = [file for file in guide_file_list if file.endswith('.mp4')][0]\n",
    "\n",
    "print(f\"Guide file list: {guide_file_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_video = cv2.VideoCapture(guide_file_list)\n",
    "guide_frame_width = int(guide_video.get(3))\n",
    "guide_frame_height = int(guide_video.get(4))\n",
    "\n",
    "guide_video_path = GUIDE_PATH + 'processed_videos_guide.mp4'\n",
    "guide_video_write = cv2.VideoWriter(\n",
    "    guide_video_path,                   # Path to the output video file\n",
    "    cv2.VideoWriter_fourcc(*'mp4v'),    # Codec to be used\n",
    "    20.0,                               # Frame rate of the video\n",
    "    (guide_frame_width, guide_frame_height) # Frame size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide Video Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting skeleton from guide video:   1%|          | 6/546.0 [00:08<12:29,  1.39s/it, FPS=0.796, Avg FPS=0.733]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guide skeleton list shape: (6, 17, 3)\n",
      "{'nose': [(528.67413, 478.09863), (528.75073, 480.20068), (528.27893, 482.25552), (528.22833, 479.69687), (526.15656, 477.87836), (526.7531, 477.4061)], 'left_eye': [(548.8595, 466.57855), (548.92865, 464.36185), (548.43884, 464.97437), (548.3768, 462.41397), (546.3343, 460.6105), (546.9019, 458.69144)], 'right_eye': [(505.60507, 466.57855), (504.24896, 464.36185), (505.23904, 464.97437), (503.76233, 462.41397), (504.53763, 460.6105), (505.16513, 458.69144)], 'left_ear': [(571.9285, 483.85864), (573.4305, 483.08044), (574.35876, 483.69562), (572.84283, 481.1371), (573.7183, 479.31735), (572.8075, 480.28528)], 'right_ear': [(478.21066, 482.41864), (473.982, 483.08044), (477.8792, 483.69562), (473.53964, 481.1371), (474.27106, 480.75632), (476.3811, 480.28528)], 'left_shoulder': [(629.6011, 587.53937), (631.0817, 591.0726), (633.3984, 584.5023), (631.84906, 587.715), (631.36884, 585.80237), (631.81476, 583.93555)], 'right_shoulder': [(417.65445, 586.09937), (414.88947, 589.6327), (417.39954, 583.0622), (415.97256, 587.715), (416.62048, 585.80237), (417.3739, 585.3752)], 'left_elbow': [(769.45703, 544.33905), (766.56226, 546.4358), (765.87775, 539.8593), (764.2533, 534.4261), (765.40643, 531.12085), (765.66034, 527.7917)], 'right_elbow': [(280.68213, 531.37897), (280.8502, 527.7172), (282.04022, 522.5782), (282.12912, 520.0236), (285.4654, 516.731), (286.40668, 511.95618)], 'left_wrist': [(756.4808, 423.3783), (753.59076, 421.165), (754.35785, 417.4512), (751.3007, 413.44577), (750.99384, 408.80698), (749.8291, 402.54755)], 'right_wrist': [(323.93655, 404.6582), (326.97122, 401.00647), (328.12, 398.72995), (329.62195, 394.72263), (330.1446, 392.97812), (332.4611, 391.03082)], 'left_hip': [(580.5794, 917.3014), (582.0782, 919.3685), (580.1187, 915.72437), (581.47784, 917.5303), (580.92456, 919.64734), (582.8819, 917.9199)], 'right_hip': [(442.16528, 915.8613), (443.71506, 913.60895), (441.8794, 914.2843), (443.3169, 910.32904), (444.00452, 911.0134), (441.8403, 909.28235)], 'left_knee': [(560.394, 1185.1431), (560.4589, 1188.6288), (558.5188, 1185.0223), (561.3294, 1186.8555), (562.1882, 1187.299), (561.29395, 1187.1227)], 'right_knee': [(447.93256, 1182.2631), (443.71506, 1184.3092), (447.63937, 1182.1421), (444.7561, 1183.975), (445.44574, 1182.9818), (446.1579, 1182.804)], 'left_ankle': [(558.9522, 1434.2645), (559.01764, 1442.0503), (558.5188, 1435.5991), (558.45105, 1441.7782), (559.30566, 1441.9996), (556.9763, 1443.3693)], 'right_ankle': [(439.28165, 1438.5846), (437.94995, 1443.4901), (438.99942, 1437.0391), (437.5602, 1443.2185), (438.23947, 1443.4386), (438.9619, 1443.3693)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_fps, frame_count = 0, 0\n",
    "guide_skeleton_list = []\n",
    "pbar = tqdm(desc=f\"Extracting skeleton from guide video\", total=guide_video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "extracted_skeletons = {}\n",
    "\n",
    "while True:\n",
    "    ret, frame = guide_video.read()\n",
    "    if ret == True:\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Get frames from video, and convert to PIL image for processing with OpenPose\n",
    "        image_from_video = np.array(frame, dtype=np.float32) / 255.0\n",
    "        original_image = deepcopy(image_from_video)\n",
    "\n",
    "        # Transform the image to tensor\n",
    "        image_from_video = torch.Tensor(image_from_video).permute(2, 0, 1)\n",
    "        image_from_video = image_from_video.unsqueeze(0).to(device)\n",
    "\n",
    "        # Get the output from the model\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = model(image_from_video)\n",
    "        latency = time.time() - start_time\n",
    "\n",
    "        # Get the keypoint from the output\n",
    "        keypoints = utils.get_keypoints(output, None, threshold=0.9)\n",
    "        guide_skeleton_list.append(keypoints)\n",
    "\n",
    "        fps = 1 / latency \n",
    "        total_fps += fps\n",
    "        frame_count += 1\n",
    "        pbar.set_postfix({\"FPS\": fps, \"Avg FPS\": total_fps / frame_count})\n",
    "\n",
    "        if frame_count == 1:    \n",
    "            for i in range(len(keypoints)): extracted_skeletons[keypoint_names[i]] = []\n",
    "\n",
    "        for i in range(len(keypoints)):\n",
    "            x, y = keypoints[i][0], keypoints[i][1]\n",
    "            extracted_skeletons[keypoint_names[i]].append((x, y))\n",
    "\n",
    "        if frame_count > 5: break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "guide_skeleton_list = np.array(guide_skeleton_list)\n",
    "guide_skeleton_list = guide_skeleton_list.astype(np.float32)\n",
    "print(f\"Guide skeleton list shape: {guide_skeleton_list.shape}\")\n",
    "print(extracted_skeletons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting skeleton from consumer video: 100%|██████████| 546/546.0 [12:50<00:00,  1.41s/it, FPS=0.765, Avg FPS=0.72] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer skeleton list shape: (546, 17, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_fps, frame_count = 0, 0\n",
    "consumer_skeleton_list = []\n",
    "pbar = tqdm(desc=f\"Extracting skeleton from consumer video\", total=guide_video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "while True:\n",
    "    ret, frame = consumer_video.read()\n",
    "    if ret == True:\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Get frames from video, and convert to PIL image for processing with OpenPose\n",
    "        image_from_video = np.array(frame, dtype=np.float32) / 255.0\n",
    "        original_image = deepcopy(image_from_video)\n",
    "\n",
    "        # Transform the image to tensor\n",
    "        image_from_video = torch.Tensor(image_from_video).permute(2, 0, 1)\n",
    "        image_from_video = image_from_video.unsqueeze(0).to(device)\n",
    "\n",
    "        # Get the output from the model\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = model(image_from_video)\n",
    "        latency = time.time() - start_time\n",
    "\n",
    "        # Get the keypoint from the output\n",
    "        keypoints = utils.get_keypoints(output, original_image, threshold=0.9)\n",
    "        consumer_skeleton_list.append(keypoints)\n",
    "\n",
    "        fps = 1 / latency \n",
    "        total_fps += fps\n",
    "        frame_count += 1\n",
    "        pbar.set_postfix({\"FPS\": fps, \"Avg FPS\": total_fps / frame_count})\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "consumer_skeleton_list = np.array(consumer_skeleton_list)\n",
    "consumer_skeleton_list = consumer_skeleton_list.astype(np.float32)\n",
    "print(f\"Consumer skeleton list shape: {consumer_skeleton_list.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Similarty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y_true = guide_skeleton_list[:530].copy()\n",
    "y_pred = consumer_skeleton_list[:530].copy()\n",
    "\n",
    "y_ture, y_pred = np.array(y_true), np.array(y_pred)\n",
    "y_true = y_true.reshape(-1, 1)\n",
    "y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "similarities = cosine_similarity(y_true, y_pred)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYMklEQVR4nO3df6zWdf3/8ccB45xTco6ZeujgMRHdsF8HUGTovm7swzojY+pa6WZJ9LWi0AK2EJaiWUm2NJ2gmbUwtambwloUjh2HZKNM8bSc0Y8PJAw9B9zsXHKyo3HO94++Xe0MjnIUPC+Ot9t2/XG9rtf7fZ7vv677Lt7XRU1/f39/AAAKNmq4BwAAeD2CBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKJ1gAgOIdNdwDHCp9fX157rnnMnbs2NTU1Az3OADAQejv789LL72U5ubmjBo1+OcoIyZYnnvuubS0tAz3GADAG7Bz586ceOKJg74+YoJl7NixSf59wQ0NDcM8DQBwMCqVSlpaWqrv44MZMcHyn38GamhoECwAcIR5vds53HQLABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8YYcLJs2bcqcOXPS3NycmpqarF279nWP2bhxY6ZOnZra2tqceuqpWb169aB7v/3tb6empiYLFy4c6mgAwAg15GDp6elJa2trVq1adVD7t2/fnvPOOy8zZ85MR0dHFi5cmMsuuywPP/zwfnt/97vf5Y477siHP/zhoY4FAIxgRw31gNmzZ2f27NkHvf/73/9+JkyYkBtvvDFJcvrpp+exxx7L9773vbS1tVX37d27N5dccknuvPPOfPOb3xzqWADACHbY72HZvHlzZs2aNWCtra0tmzdvHrC2YMGCnHfeefvtHUxvb28qlcqABwAwMg35E5ah6uzsTFNT04C1pqamVCqVvPzyy6mvr899992XLVu25He/+91Bn3fFihX5+te/fqjHBQAKNOzfEtq5c2e+8pWv5N57701dXd1BH7ds2bJ0d3dXHzt37jyMUwIAw+mwf8Iybty4dHV1DVjr6upKQ0ND6uvr8+STT2b37t2ZOnVq9fV9+/Zl06ZNWblyZXp7ezN69Oj9zltbW5va2trDPT4AUIDDHiwzZszIL37xiwFrGzZsyIwZM5Ik//M//5M//OEPA16fN29eJk2alCuvvPKAsQIAvL0MOVj27t2bv/71r9Xn27dvT0dHR4499ticdNJJWbZsWXbt2pWf/OQnSZL58+dn5cqVWbJkST772c/mkUceyQMPPJB169YlScaOHZsPfvCDA/7Gu971rrznPe/Zbx0AeHsa8j0sTzzxRKZMmZIpU6YkSRYvXpwpU6Zk+fLlSZLnn38+O3bsqO6fMGFC1q1blw0bNqS1tTU33nhjfvjDHw74SjMAwGup6e/v7x/uIQ6FSqWSxsbGdHd3p6GhYbjHAQAOwsG+fw/7t4QAAF6PYAEAiidYAIDiCRYAoHiCBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKJ1gAgOIJFgCgeIIFACieYAEAiidYAIDiCRYAoHiCBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKJ1gAgOIJFgCgeIIFACieYAEAiidYAIDiCRYAoHiCBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKJ1gAgOIJFgCgeIIFACieYAEAiidYAIDiCRYAoHiCBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKJ1gAgOIJFgCgeIIFACieYAEAiidYAIDiCRYAoHiCBQAonmABAIonWACA4gkWAKB4Qw6WTZs2Zc6cOWlubk5NTU3Wrl37usds3LgxU6dOTW1tbU499dSsXr16wOsrVqzItGnTMnbs2Jxwwgm54IIL8qc//WmoowEAI9SQg6Wnpyetra1ZtWrVQe3fvn17zjvvvMycOTMdHR1ZuHBhLrvssjz88MPVPY8++mgWLFiQ3/zmN9mwYUNeffXVfOQjH0lPT89QxwMARqCa/v7+/jd8cE1N1qxZkwsuuGDQPVdeeWXWrVuXp59+urp28cUX5+9//3vWr19/wGP27NmTE044IY8++mjOPffcg5qlUqmksbEx3d3daWhoGNJ1AADD42Dfvw/7PSybN2/OrFmzBqy1tbVl8+bNgx7T3d2dJDn22GMH3dPb25tKpTLgAQCMTIc9WDo7O9PU1DRgrampKZVKJS+//PJ++/v6+rJw4cKcc845+eAHPzjoeVesWJHGxsbqo6Wl5ZDPDgCUobhvCS1YsCBPP/107rvvvtfct2zZsnR3d1cfO3fufIsmBADeakcd7j8wbty4dHV1DVjr6upKQ0ND6uvrB6xffvnl+fnPf55NmzblxBNPfM3z1tbWpra29pDPCwCU57B/wjJjxoy0t7cPWNuwYUNmzJhRfd7f35/LL788a9asySOPPJIJEyYc7rEAgCPIkINl79696ejoSEdHR5J/f225o6MjO3bsSPLvf6q59NJLq/vnz5+fbdu2ZcmSJdm6dWtuu+22PPDAA1m0aFF1z4IFC3LPPffkpz/9acaOHZvOzs50dnYe8B4XAODtZ8hfa964cWNmzpy53/rcuXOzevXqfOYzn8nf/va3bNy4ccAxixYtyjPPPJMTTzwxV199dT7zmc/8d4iamgP+rR//+McD9r0WX2sGgCPPwb5/v6nfYSmJYAGAI08xv8MCAPBmCRYAoHiCBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKJ1gAgOIJFgCgeIIFACieYAEAiidYAIDiCRYAoHiCBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKJ1gAgOIJFgCgeIIFACieYAEAiidYAIDiCRYAoHiCBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKJ1gAgOIJFgCgeIIFACieYAEAiidYAIDiCRYAoHiCBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKJ1gAgOIJFgCgeIIFACieYAEAiidYAIDiCRYAoHiCBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKN+Rg2bRpU+bMmZPm5ubU1NRk7dq1r3vMxo0bM3Xq1NTW1ubUU0/N6tWr99uzatWqnHzyyamrq8v06dPz+OOPD3U0AGCEGnKw9PT0pLW1NatWrTqo/du3b895552XmTNnpqOjIwsXLsxll12Whx9+uLrn/vvvz+LFi3PNNddky5YtaW1tTVtbW3bv3j3U8QCAEaimv7+//w0fXFOTNWvW5IILLhh0z5VXXpl169bl6aefrq5dfPHF+fvf/57169cnSaZPn55p06Zl5cqVSZK+vr60tLTkiiuuyNKlSw9qlkqlksbGxnR3d6ehoeGNXhIA8BY62Pfvow73IJs3b86sWbMGrLW1tWXhwoVJkldeeSVPPvlkli1bVn191KhRmTVrVjZv3jzoeXt7e9Pb21t9XqlUDu3g8Db0wvM786s1Pzok5/rHP3ryv/+77ZCc61CbOPGUvPOd73rT5xk/vjlnzf5UMuadh2Aq4LUc9mDp7OxMU1PTgLWmpqZUKpW8/PLLefHFF7Nv374D7tm6deug512xYkW+/vWvH5aZ4e3qV2t+lAt3f+/QnbDp9bcMi73///Fm7U62H39CJpx9wSE4GfBaDnuwHC7Lli3L4sWLq88rlUpaWlqGcSI48v2fC/9v1qw5NOd623zCcuZHDsFEwOs57MEybty4dHV1DVjr6upKQ0ND6uvrM3r06IwePfqAe8aNGzfoeWtra1NbW3tYZoa3q+Pe25ILv3TtcI8BsJ/D/jssM2bMSHt7+4C1DRs2ZMaMGUmSMWPG5Iwzzhiwp6+vL+3t7dU9AMDb25CDZe/eveno6EhHR0eSf39tuaOjIzt27Ejy73+qufTSS6v758+fn23btmXJkiXZunVrbrvttjzwwANZtGhRdc/ixYtz55135q677sof//jHfPGLX0xPT0/mzZv3Ji8PABgJhvxPQk888URmzpxZff6f+0jmzp2b1atX5/nnn6/GS5JMmDAh69aty6JFi3LLLbfkxBNPzA9/+MO0tbVV91x00UXZs2dPli9fns7OzkyePDnr16/f70ZcAODt6U39DktJ/A4LABx5Dvb92/8lBAAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQvDcULKtWrcrJJ5+curq6TJ8+PY8//vige1999dVcd911mThxYurq6tLa2pr169cP2LNv375cffXVmTBhQurr6zNx4sR84xvfSH9//xsZDwAYYYYcLPfff38WL16ca665Jlu2bElra2va2tqye/fuA+6/6qqrcscdd+TWW2/NM888k/nz5+fCCy/MU089Vd1zww035Pbbb8/KlSvzxz/+MTfccEO+853v5NZbb33jVwYAjBg1/UP8GGP69OmZNm1aVq5cmSTp6+tLS0tLrrjiiixdunS//c3Nzfna176WBQsWVNc+/vGPp76+Pvfcc0+S5GMf+1iampryox/9aNA9r6dSqaSxsTHd3d1paGgYyiUBAMPkYN+/h/QJyyuvvJInn3wys2bN+u8JRo3KrFmzsnnz5gMe09vbm7q6ugFr9fX1eeyxx6rPzz777LS3t+fPf/5zkuT3v/99HnvsscyePXso4wEAI9RRQ9n8wgsvZN++fWlqahqw3tTUlK1btx7wmLa2ttx0000599xzM3HixLS3t+ehhx7Kvn37qnuWLl2aSqWSSZMmZfTo0dm3b1++9a1v5ZJLLhl0lt7e3vT29lafVyqVoVwKAHAEOezfErrlllty2mmnZdKkSRkzZkwuv/zyzJs3L6NG/fdPP/DAA7n33nvz05/+NFu2bMldd92V7373u7nrrrsGPe+KFSvS2NhYfbS0tBzuSwEAhsmQguW4447L6NGj09XVNWC9q6sr48aNO+Axxx9/fNauXZuenp48++yz2bp1a44++uiccsop1T1f/epXs3Tp0lx88cX50Ic+lE9/+tNZtGhRVqxYMegsy5YtS3d3d/Wxc+fOoVwKAHAEGVKwjBkzJmeccUba29ura319fWlvb8+MGTNe89i6urqMHz8+//rXv/Lggw/m/PPPr772j3/8Y8AnLkkyevTo9PX1DXq+2traNDQ0DHgAACPTkO5hSZLFixdn7ty5OfPMM3PWWWfl5ptvTk9PT+bNm5ckufTSSzN+/PjqpyO//e1vs2vXrkyePDm7du3Ktddem76+vixZsqR6zjlz5uRb3/pWTjrppHzgAx/IU089lZtuuimf/exnD9FlAgBHsiEHy0UXXZQ9e/Zk+fLl6ezszOTJk7N+/frqjbg7duwY8GnJP//5z1x11VXZtm1bjj766Hz0ox/N3XffnWOOOaa659Zbb83VV1+dL33pS9m9e3eam5vzhS98IcuXL3/zVwgAHPGG/DsspfI7LABw5Dksv8MCADAcBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPHeULCsWrUqJ598curq6jJ9+vQ8/vjjg+599dVXc91112XixImpq6tLa2tr1q9fv9++Xbt25VOf+lTe8573pL6+Ph/60IfyxBNPvJHxAIARZsjBcv/992fx4sW55pprsmXLlrS2tqatrS27d+8+4P6rrroqd9xxR2699dY888wzmT9/fi688MI89dRT1T0vvvhizjnnnLzjHe/IL3/5yzzzzDO58cYb8+53v/uNXxkAMGLU9Pf39w/lgOnTp2fatGlZuXJlkqSvry8tLS254oorsnTp0v32Nzc352tf+1oWLFhQXfv4xz+e+vr63HPPPUmSpUuX5te//nV+9atfveELqVQqaWxsTHd3dxoaGt7weQCAt87Bvn8P6ROWV155JU8++WRmzZr13xOMGpVZs2Zl8+bNBzymt7c3dXV1A9bq6+vz2GOPVZ//7Gc/y5lnnplPfOITOeGEEzJlypTceeedrzlLb29vKpXKgAcAMDINKVheeOGF7Nu3L01NTQPWm5qa0tnZecBj2tractNNN+Uvf/lL+vr6smHDhjz00EN5/vnnq3u2bduW22+/PaeddloefvjhfPGLX8yXv/zl3HXXXYPOsmLFijQ2NlYfLS0tQ7kUAOAIcti/JXTLLbfktNNOy6RJkzJmzJhcfvnlmTdvXkaN+u+f7uvry9SpU3P99ddnypQp+fznP5/Pfe5z+f73vz/oeZctW5bu7u7qY+fOnYf7UgCAYTKkYDnuuOMyevTodHV1DVjv6urKuHHjDnjM8ccfn7Vr16anpyfPPvtstm7dmqOPPjqnnHJKdc973/vevP/97x9w3Omnn54dO3YMOkttbW0aGhoGPACAkWlIwTJmzJicccYZaW9vr6719fWlvb09M2bMeM1j6+rqMn78+PzrX//Kgw8+mPPPP7/62jnnnJM//elPA/b/+c9/zvve976hjAcAjFBHDfWAxYsXZ+7cuTnzzDNz1lln5eabb05PT0/mzZuXJLn00kszfvz4rFixIkny29/+Nrt27crkyZOza9euXHvttenr68uSJUuq51y0aFHOPvvsXH/99fnkJz+Zxx9/PD/4wQ/ygx/84BBdJgBwJBtysFx00UXZs2dPli9fns7OzkyePDnr16+v3oi7Y8eOAfen/POf/8xVV12Vbdu25eijj85HP/rR3H333TnmmGOqe6ZNm5Y1a9Zk2bJlue666zJhwoTcfPPNueSSS978FQIAR7wh/w5LqfwOCwAceQ7L77AAAAwHwQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABRPsAAAxRMsAEDxBAsAUDzBAgAUT7AAAMUTLABA8QQLAFA8wQIAFE+wAADFEywAQPEECwBQPMECABTvqOEe4FDp7+9PklQqlWGeBAA4WP953/7P+/hgRkywvPTSS0mSlpaWYZ4EABiql156KY2NjYO+XtP/eklzhOjr68tzzz2XsWPHpqamZrjHAQ6hSqWSlpaW7Ny5Mw0NDcM9DnAI9ff356WXXkpzc3NGjRr8TpUREyzAyFWpVNLY2Jju7m7BAm9TbroFAIonWACA4gkWoHi1tbW55pprUltbO9yjAMPEPSwAQPF8wgIAFE+wAADFEywAQPEECwBQPMECFGvTpk2ZM2dOmpubU1NTk7Vr1w73SMAwESxAsXp6etLa2ppVq1YN9yjAMBsx//khMPLMnj07s2fPHu4xgAL4hAUAKJ5gAQCKJ1gAgOIJFgCgeIIFACiebwkBxdq7d2/++te/Vp9v3749HR0dOfbYY3PSSScN42TAW83/1gwUa+PGjZk5c+Z+63Pnzs3q1avf+oGAYSNYAIDiuYcFACieYAEAiidYAIDiCRYAoHiCBQAonmABAIonWACA4gkWAKB4ggUAKJ5gAQCKJ1gAgOIJFgCgeP8PxpsFe/63CiIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(similarities.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000383235683\n"
     ]
    }
   ],
   "source": [
    "similarities = similarities.flatten()\n",
    "similarities = np.sum(similarities, axis=0) / len(similarities)\n",
    "\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(x, y):\n",
    "    return np.mean((x - y) ** 2)\n",
    "\n",
    "def mean_absolute_error(x, y):\n",
    "    return np.mean(np.abs(x - y))\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "def manhattan_distance(x, y):\n",
    "    return np.sum(np.abs(x - y))\n",
    "\n",
    "def chebyshev_distance(x, y):\n",
    "    return np.max(np.abs(x - y))\n",
    "\n",
    "def minkowski_distance(x, y, p):\n",
    "    return np.sum(np.abs(x - y) ** p) ** (1 / p)\n",
    "\n",
    "def jaccard_similarity(x, y):\n",
    "    return np.sum(np.min([x, y], axis=0)) / np.sum(np.max([x, y], axis=0))\n",
    "\n",
    "def hamming_distance(x, y):\n",
    "    return np.sum(x != y)\n",
    "\n",
    "def kl_divergence(x, y):\n",
    "    return np.sum(x * np.log(x / y))\n",
    "\n",
    "def hellinger_distance(x, y):\n",
    "    return np.sqrt(np.sum((np.sqrt(x) - np.sqrt(y)) ** 2)) / np.sqrt(2)\n",
    "\n",
    "def bhattacharyya_distance(x, y):\n",
    "    return -np.log(np.sum(np.sqrt(x * y)))\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "def log_cosh(x, y):\n",
    "    return np.log(np.cosh(x - y))\n",
    "\n",
    "function_list = [\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    cosine_similarity,\n",
    "    euclidean_distance,\n",
    "    manhattan_distance,\n",
    "    chebyshev_distance,\n",
    "    minkowski_distance,\n",
    "    jaccard_similarity,\n",
    "    hamming_distance,\n",
    "    kl_divergence,\n",
    "    hellinger_distance,\n",
    "    bhattacharyya_distance,\n",
    "    pearson_correlation,\n",
    "    log_cosh\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "mean_squared_error\n",
      "0.0\n",
      "-----------------------\n",
      "mean_absolute_error\n",
      "0.0\n",
      "-----------------------\n",
      "euclidean_distance\n",
      "0.0\n",
      "-----------------------\n",
      "manhattan_distance\n",
      "0.0\n",
      "-----------------------\n",
      "chebyshev_distance\n",
      "0.0\n",
      "-----------------------\n",
      "jaccard_similarity\n",
      "1.0\n",
      "-----------------------\n",
      "hamming_distance\n",
      "0\n",
      "-----------------------\n",
      "kl_divergence\n",
      "0.0\n",
      "-----------------------\n",
      "hellinger_distance\n",
      "0.0\n",
      "-----------------------\n",
      "bhattacharyya_distance\n",
      "-16.244684\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/numpy/lib/function_base.py:2704: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/numpy/lib/function_base.py:2704: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_correlation\n",
      "nan\n",
      "-----------------------\n",
      "log_cosh\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "print('-----------------------')\n",
    "for fuc in function_list:\n",
    "    try: \n",
    "        prediction = fuc(y_true, y_pred)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    print(fuc.__name__)\n",
    "    print(prediction)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
